{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Multielectrode Data in pandas\n",
    "\n",
    "If you haven't already, please read the [description](./intro_multielec_data) of the experiment that this data comes from, in the previous section.\n",
    "\n",
    "We're going to load in the multielectrode array data from a CSV file, do some EDA, and make some inferences with respect to our experimental questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set known experiment parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# times the stimulus went on and off\n",
    "trial_start_time = -0.150\n",
    "grating_on_time  = 0.0\n",
    "grating_off_time = 2.0\n",
    "trial_end_time   = 2.5\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "df = pd.read_csv('data/multielectrode_data.csv')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data \n",
    "\n",
    "\n",
    "These data are again in **long format**, and they are sparse data with one row for each spike. Let's look at the head of the data to get oriented:\n",
    "~~~python\n",
    "df.head()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "- channel — which electrode the data came from\n",
    "- time — spike time, relative to stimulus onset (so we have negative values for spikes that occurred during the fixation period prior to stimulus onset). This is measured in seconds\n",
    "- orientation — of stimulus (0 or 90 deg)\n",
    "- trial number — 1150 trials for each orientation\n",
    "\n",
    "We can see how many rows there are in the DataFrame (as well as the number of columns, but we could already see that in this case):\n",
    "~~~python\n",
    "df.shape\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "df.shape[0] / 20 / 2\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Electrodes\n",
    "Let's see how many electrodes we have data from, and what their labels are. We save each as a variable, which will come in handy later in looping through these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "num_chan = len(df['channel'].unique())\n",
    "print('Number of electrodes (channels): ' + str(num_chan))\n",
    "\n",
    "channels = sorted(df['channel'].unique())  # use the sorted() function so the channels are listed sequentially\n",
    "print('Channel labels: ' + str(channels))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit weird — we're told this is a 96 electrode array, but there are only 15 electrodes?!  \n",
    "\n",
    "This is because the full data set is huge, with over 2 million rows. The amount of memory that this requires makes doing anything with the data quite slow. So we've provided data for a subset of channels for the purposes of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientations\n",
    "What about orientations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "orientations = sorted(df['orientation'].unique())\n",
    "num_ortns = len(orientations)\n",
    "print('Found ' + str(num_ortns) + ' orientations, which are: ' + str(orientations))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most channels contain activity from one neuron, but channel 20 picked up 4 neurons, and 29 and 60 each picked up 2 neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Peri-Stimulus Time Histograms (PSTH)\n",
    "\n",
    "Since we have thousands of trials, we're not going to plot rasters for this data set. Since PSTHs collapse across trials, this is a more convenient and compact way of representing spiking activity as a function of time. \n",
    "\n",
    "Let's start by plotting the PSTH for one channel, #4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "dat = df[(df['channel'] == 4)]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pandas' `.hist()` method to obtain a PSTH, firstly just across all trials and electrodes/neurons, with the following arguments:\n",
    "- `column='time'` specifies that the values in this column (spike times) are used to generate the histogram\n",
    "- the `by='orientation'` argument generates separate plots for each value in the orientation column\n",
    "- the `bins=time_bins` variable we set earlier defines the x axis of the histogram (`bins`); `time_bins` is in 10 ms increments so that becomes the size of our histogram bins\n",
    "- `sharey` forces the y axis range to be the same across plots\n",
    "- `layout` specifies 2 rows (each orientation) and one column\n",
    "- `figsize` was determined by trial and error to provide a subjectively-nice aspect ratio and fit the figure in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bin widths\n",
    "One thing we need to do before creating any histograms is defining our bin widths. Although the histogram function will do this automatically, for PSTHs we want more control over the bins, specifically so that each bin corresponds to a 10 ms period. \n",
    "\n",
    "Recall that the spike times are in seconds, so we use 0.01 as the step size to get 10 ms bins, going from the start to the end of the trial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# Create a set of 10 ms bins for our PSTHs, spanning the time range of each trial\n",
    "time_bins = np.arange(trial_start_time, trial_end_time, 0.01)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the PSTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 8], sharey=True)\n",
    "\n",
    "for idx, ori in enumerate(orientations):\n",
    "    axs[idx].hist(dat[dat['orientation'] == ori]['time'], \n",
    "                 bins=time_bins)\n",
    "\n",
    "plt.show()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both orientations overlaid\n",
    "\n",
    "For this we use the kwarg `histtype='step'` to draw the histograms as lines rather than bars:\n",
    "~~~python\n",
    "fig, axs = plt.subplots(figsize=[10, 4], sharey=True)\n",
    "\n",
    "for idx, ori in enumerate(orientations):\n",
    "    axs.hist(dat[dat['orientation'] == ori]['time'], \n",
    "                  bins=time_bins,\n",
    "                  histtype='step' \n",
    "                 )\n",
    "    \n",
    "plt.show()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make array of histograms\n",
    "\n",
    "The above examples were OK for one channel, but it would be nice to have more control over the plots and be able to easily work with all of the data at once. \n",
    "\n",
    "To facilitate this, we'll make a NumPy array of histograms similar to what we did in the heat maps lesson. However, in this case the rows of the array are not intensity levels, but each combination of channel and orientation. We're not going to use this array to plot heat maps (since the rows here aren't levels of a continuous variable), but just to store the histograms to make it easy to plot them later.\n",
    "\n",
    "After we make the NumPy array we convert it into a pandas DataFrame, which allows us to make the column labels the actual time points that each bin starts with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "df_list = []\n",
    "\n",
    "# Initialize histogram array\n",
    "# bins are edges, so there is 1 less value in histogram than in bins\n",
    "hist_arr = np.zeros([num_chan * num_ortns, \n",
    "                     len(time_bins)-1]  \n",
    "                   )\n",
    "\n",
    "# initialize row index variable to increase each time through the loop\n",
    "r_idx = 0\n",
    "\n",
    "for chan in channels:\n",
    "    for ori in orientations:\n",
    "        hist_arr[r_idx], bins = np.histogram(df[(df['channel']==chan) & (df['orientation']==ori)]['time'],\n",
    "                                             bins=time_bins)\n",
    "        r_idx += 1 # increment counter for next row\n",
    "        \n",
    "# Convert to pandas DataFrame with time values as column labels   \n",
    "# We'll round the time values so that they don't have reallly long decimal places\n",
    "df_psth = pd.DataFrame(hist_arr,\n",
    "                      columns=np.round(time_bins[:-1], 3))        \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "df_psth.head()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns of labels\n",
    "Next we'll add columns for channel and orientation labels.\n",
    "\n",
    "To make this DataFrame, first we'll create a DataFrame of the two columns of labels we want (channel and orientation) and then we'll concatenate that with the NumPy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "labels_tmp = []\n",
    "\n",
    "for chan in channels:\n",
    "    for ori in orientations:\n",
    "        labels_tmp.append([chan, ori])\n",
    "        \n",
    "condition_labels = pd.DataFrame(labels_tmp, \n",
    "                                columns=['channel', 'orientation'])\n",
    "\n",
    "condition_labels.head(8)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can merge the labels and histograms with `pd.concat()`. The first argument to `pd.concat()` is a list of the DataFrames you want to merge, and the `axis=1` kwarg is used to specify that we want to merge by columns (add the two DataFrames side-by-side, instead of one on top of the other).\n",
    "\n",
    "Note that we don't assign the output to a new variable, we just view the head, because we're actually going to make the final version of this DataFrame a different way below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "pd.concat([condition_labels, df_psth], axis=1).head()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced approach: Using list comprehension\n",
    "\n",
    "The above steps work fine, but just for fun, here's a tricky but elegant *nested list comprehension* way of doing this. Recall that a list comprehension is essentially a `for` loop inside a set of square brackets, that creates a list. But just as we can nest one `for` loop inside another, we can nest `for` statements inside list comprehensions, to create a list of lists, and then convert that to a pandas DataFrame, all in one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "condition_labels = pd.DataFrame([[chan, ori] for chan in channels for ori in orientations ],\n",
    "                               columns=['channel', 'orientation'])\n",
    "\n",
    "condition_labels.head(8)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add labels to histograms DataFrame with list comprehension\n",
    "\n",
    "The cool thing about the list comprehension approach is that we don't need to separately make a DataFrame of labels and then merge that with the DataFrame of histograms; instead, we can just pass the list comprehension as two new channels of the DataFrame, all in one step. \n",
    "\n",
    "This works because the list comprehension runs in a single line of code, and because pandas will interpret each entry in a list of lists as a row in a DataFrame. We just have to be sure to assign the results of the list comprehension to a list of column labels. That is, below we use \n",
    "\n",
    "    df_psth[['channel', 'orientation']]\n",
    "\n",
    "and not \n",
    "\n",
    "    df_psth['channel', 'orientation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "#So we will use:\n",
    "df_psth[['channel', 'orientation']] = [[chan, ori] for chan in channels for ori in orientations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we have wide-format data here, and the two columns we just added are labels, we will make these two columns indexes of the DataFrame. This will make working with it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "df_psth = df_psth.set_index(['channel', 'orientation'])\n",
    "df_psth.head()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting the Histograms\n",
    "\n",
    "Now we can plot the histograms that we just completed. These should look similar to the panel of 8 histograms we plotted above; the big difference is that above we use pandas `.histogram()` method, whereas here we pre-computed the histograms so that we can use Matplotlib's more flexible and general-purpose `.plot()` function. This allows us more control and power in drawing the plot. For example, below we shade in the time period when the stimulus was on, and we draw the PSTHs as lines rather than bars. Although this may seem like something of an aesthetic preference, as you'll see below, lines give you the ability to overlay PSTHs for different conditions, allowing comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# We know there are 20 channels so define a 5 x 4 array of subplots\n",
    "fig, axs = plt.subplots(5, 4, figsize=[15, 10])\n",
    "\n",
    "# Use enumerate to loop through channels\n",
    "for chan, ax in enumerate(axs.reshape(-1)):\n",
    "    # 0 deg orienation\n",
    "    ax.plot(time_bins[:-1], \n",
    "           df_psth.loc[(channels[chan], orientations[0])]\n",
    "           )\n",
    "\n",
    "    # 90 deg orientation\n",
    "    ax.plot(time_bins[:-1], \n",
    "           df_psth.loc[(channels[chan], orientations[1])],\n",
    "           color='red'\n",
    "           )\n",
    "\n",
    "    ax.axvspan(grating_on_time, grating_off_time, alpha=0.5, color='lightgray')\n",
    "    ax.set_title('Channel ' + str(chan))\n",
    "    ax.set_ylabel('Spike count per bin')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "      \n",
    "plt.tight_layout() \n",
    "plt.show()    \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above the differences in total spike counts (*y* axis ranges) between sort codes. This is because the data are summed over channels; every channel has at least a sort code of 1 (i.e., contains data from one neuron/unit), but fewer channels were found to contain data from two or more units. So decreasing amounts of data contribute to the averages for sort codes 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you'll note above is that there are some missing panels, including channels 21, 31, 34, 56, and 68. As noted above, these are the dead channels. The way the code above is written, we use the channel number as the index for the subplot position. Since there is no channel in the `channels` list for the dead channels, those subplot panels are simply left blank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond that, it's clear that different channels show very different response profiles. Some (e.g., 1, 2, 4, 5) show clear transient peaks in spiking activity soon after stimulus onset. Others do not. Some of the channels that show this initial peak return to baseline (low) levels of firing soon after, while others (e.g., 1, 20, 72) show a sustained plateau as long as the stimulus is on. Others (e.g., 3, 62) show no initial peak, but a steady rise in firing rate as the stimulus duration increases. As well, a few channels show orientation sensitivity, with clearly different responses between 0 (blue) and 90 (red) degree gratings (e.g., 29, 63, 75, 76, 91)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
